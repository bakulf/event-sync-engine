# Event Sync Engine - Architecture Documentation

> This document describes the architecture and design principles implemented in the Event Sync Engine.
>
> Originally designed for [Firefox Multi-Account Containers](https://github.com/mozilla/multi-account-containers), this engine provides a generic, reusable solution for distributed event sourcing sync in browser extensions using the WebExtension storage.sync API.

## Problem Statement

The current sync implementation using `storage.sync` API is fragile due to inherent limitations:

1. **No conflict resolution**: Last server write wins, no merge semantics
2. **No atomicity**: Write operations are not real-time, sync happens every 10+ minutes
3. **Race conditions**: Any shared writable key creates potential data loss
4. **No ordering guarantees**: Cannot reliably establish operation sequence across devices

This RFC proposes a distributed event sourcing architecture that works within `storage.sync` constraints.

---

## Architecture Principles

1. **Event sourcing**: All changes are immutable events with deterministic ordering
2. **No shared writable keys**: Each device writes only to its own namespace
3. **Vector clock**: Track last seen increment per device for deduplication
4. **Hybrid Logical Clock (HLC)**: Global deterministic event ordering despite clock drift

---

## Core Concepts

### Event sourcing

A common mistake when using `storage.sync` is to store the local data structure directly. When a merge is needed, the code tries to guess how to reconcile the local state with the remote one (or vice versa). This is buggy because there is no information about how the remote data structure was generated or what events led to that state. The merge algorithm must answer tricky questions like: "Locally I don't have container X, but it exists in sync. Should I create it locally or delete it remotely?", "What wins between local and remote data?". Either way, data will be lost.

Another issue is the **last-write-wins semantics** that `storage.sync` uses. There is no versioning, no way to say: write this only if nothing has changed in the meantime.

The proposed approach is event sourcing. This means we store in sync the sequence of events to apply on top of a common baseline. When syncing, we find the last known event and apply those that follow. State is not stored directly; it is computed by applying events. Every device applies the same events in the same order, reaching the same final state.

### No shared writable keys 

Each device writes to its own key. No shared writable key means no race conditions. The events are immutable and they have to be applied in the correct order using a Hybrid Logical Clock (HLC).

### Vector clock

Each device maintains a map of the last seen increment from every other device. For example, device A tracks: `{device_B: 50, device_C: 30}`, meaning it has processed events from B up to increment 50 and from C up to increment 30. This allows deduplication: when syncing, device A fetches only events with increment > 50 from B and > 30 from C. Without vector clocks, devices would need to store all events forever or risk re-processing duplicates. The vector clock is stored locally (not in sync) and updated after each sync.

### Hybrid Logical Clock (HLC)

Events from different devices need a total ordering to be applied consistently. Physical timestamps (`Date.now()`) are unreliable due to clock drift and skew between devices. HLC combines physical time with a logical counter: when generating an event, if physical time has advanced, use it and reset counter to 0; otherwise increment the counter. When receiving remote events, adjust local HLC to be higher than any seen timestamp. This creates a monotonically increasing clock that approximates real time but guarantees causality. Events are ordered by: `hlc_time` first, then `hlc_counter`, then `device_id` (for deterministic tiebreaking). Every device applying events in HLC order reaches the same final state.

---

## Data Structures

### Storage.sync Keys

**Note:** Throughout this document, `device_id` refers to the UUID extracted from the key name. For example, in key `m_uuid-device-A`, the device_id is `uuid-device-A`. Device IDs are not stored redundantly in the data structures.

#### Meta Key: `m_<UUID>`

Each device creates a *meta* key to describe itself. Written when the device generates new events. The device ID is the `<UUID>` part of the key name.

```json
{
  "version": 1,
  "last_increment": 100,
  "shards": [0, 1, 2]
}
```

- `version`: Protocol version number (required). Indicates which protocol version this device supports. All new devices must include this field.
- `last_increment`: Highest event increment generated by this device
- `shards`: Array of active event shard indices. Used to know which `e_<UUID>_X` keys exist (GC may delete old shards)

#### Seen Key: `s_<UUID>`

Vector clock tracking what each device has seen from other devices, with activity tracking. Written during sync, **separate from meta to avoid unnecessary sync notifications**.

```json
{
  "increments": {
    "uuid-device-B": 50,
    "uuid-device-C": 30
  },
  "lastActive": 1707649200000
}
```

- `increments`: Maps remote device IDs to the last increment this device has processed from them. Does NOT include own device_id.
- `lastActive`: Timestamp (milliseconds) of when this device last updated its activity. Updated once per day during sync to track device liveness for garbage collection of inactive devices.

**Design Rationale: Why separate from `m_*`?**

The `s_*` key is intentionally separate from `m_*` to prevent sync notification loops:
- `m_*` changes → signals other devices: "I have new events, sync with me"
- `s_*` changes → internal bookkeeping, does NOT trigger sync on other devices

If `s_*` data were stored in `m_*`, every sync would trigger `m_*` changes, which would notify all other devices to sync, creating an infinite loop of sync notifications. By keeping `s_*` separate, devices can update their vector clocks and activity timestamps without broadcasting unnecessary sync notifications to the network.

This separation means:
- `m_*` is modified ONLY when generating new events (incrementing `last_increment`)
- `s_*` can be updated freely during sync without causing cascading sync operations
- Activity tracking (`lastActive`) updates don't spam the network with sync triggers

#### Event Keys: `e_<UUID>_<X>`

Array of events, sharded by sequence number X (0, 1, 2...) to work around 8KB per-key limit. The device ID is the `<UUID>` part of the key name.

```json
[
  {
    "increment": 95,
    "hlc_time": 1707649100000,
    "hlc_counter": 0,
    "op": {
      "type": "container:create",
      "data": "{\"name\":\"Work\",\"color\":\"blue\",\"icon\":\"briefcase\"}"
    }
  },
  {
    "increment": 100,
    "hlc_time": 1707649200000,
    "hlc_counter": 0,
    "op": {
      "type": "document:save",
      "chunks": 3,
      "fromChunk": 0
    }
  }
]
```

**Event Operation Structure:**
- `type`: Event type identifier (required)
- `data`: Event payload as JSON string (present if not chunked)
- `chunks`: Number of chunks (present if data is chunked)
- `fromChunk`: Starting chunk offset in shard (present if chunked)

**Data Serialization:**
- All event data is automatically serialized as JSON strings
- Event handlers receive deserialized objects (automatic JSON.parse)
- Data >7KB is automatically chunked

**Chunking:**
- Large events (>7KB) are split into chunks stored as separate keys: `e_<UUID>_<X>_<N>`
- Multiple chunked events can coexist in the same shard with different `fromChunk` offsets
- Chunks are transparently reconstructed during sync
- Example: Event with `chunks: 3, fromChunk: 0` uses chunks `e_device-A_0_0`, `e_device-A_0_1`, `e_device-A_0_2`
- Next chunked event in same shard with `chunks: 2, fromChunk: 3` uses chunks `e_device-A_0_3`, `e_device-A_0_4`

**Note:** When sorting events from multiple devices, the device ID is extracted from the key name (`e_<UUID>_<X>`) and used for HLC ordering (time, counter, device_id).

#### Baseline Key: `b_<UUID>`

Snapshot of device state with metadata about which events from each device are included in this snapshot. All data is automatically serialized as JSON with automatic chunking for large data.

```json
{
  "includes": {
    "uuid-device-A": 90,
    "uuid-device-B": 30,
    "uuid-device-C": 20
  },
  "state": "{\"containers\":{\"container-uuid\":{\"name\":\"Personal\",\"color\":\"red\",\"icon\":\"circle\"}},\"sites\":{\"container-uuid\":[\"google.com\",\"github.com\"]}}"
}
```

**Baseline Structure:**
- `includes`: Map of device IDs to last increment included in this baseline. When bootstrapping, apply events with `increment > includes[device_id]` for each known device. If a device is not in this map, it's newer than the baseline - apply all its events (`increment > 0`).
- `state`: Baseline data as JSON string (present if not chunked)
- `chunks`: Number of chunks (present if data is chunked)

**Data Serialization:**
- All baseline data is automatically serialized as JSON strings
- Baseline handlers receive deserialized state objects (automatic JSON.parse)
- Data >7KB is automatically chunked

**Chunking:**
- Large baselines are split into chunks stored as: `b_<UUID>_0`, `b_<UUID>_1`, etc.
- Chunks are transparently reconstructed during bootstrap
- Garbage collection removes orphaned chunks when baseline shrinks

### In-Memory State

#### Known Increments: `known_increments`

Kept in memory and persisted to `s_<UUID>.increments` in storage.sync.

```json
{
  "uuid-device-B": 150,
  "uuid-device-C": 200
}
```

Vector clock tracking the last processed increment from each remote device. This map stores the highest increment we have seen and applied from each device during previous sync operations. During the next sync, we process only new events (events with increment greater than the value stored in this map for that device). Does NOT include own device_id. Restored from `s_<UUID>.increments` on initialization.

#### Device State: `device_state`

Kept in memory. Critical fields (`last_increment`, `shards`) are persisted to `m_<UUID>` in storage.sync.

```json
{
  "device_id": "uuid-device-A",
  "last_increment": 100,
  "hlc_time": 1707649200000,
  "hlc_counter": 5,
  "current_shard": 0,
  "events_since_baseline_update": 7
}
```

- `device_id`: This device's unique identifier
- `last_increment`: Highest event increment generated by this device
- `hlc_time`: Current Hybrid Logical Clock physical time
- `hlc_counter`: Current HLC logical counter
- `current_shard`: Local pointer to current event shard index being written to (e.g., 0 means writing to `e_<UUID>_0`). Incremented when shard exceeds ~7KB. Source of truth for which shards exist is `meta.shards` in storage.sync.
- `events_since_baseline_update`: Counter of events written since last baseline update. Reset to 0 when baseline is updated (threshold: 15 events).

---

## Protocol Versioning

### Design Philosophy

The sync protocol is designed to be **always backward-compatible**. This means:
- New versions of the engine can read and process data from older versions
- The system evolves forward while maintaining compatibility with existing devices
- Version checks ensure devices can interoperate safely

### Version Storage

Each device stores its protocol version in the `Meta` key (`m_<UUID>`):

```json
{
  "version": 1,
  "last_increment": 100,
  "shards": [0, 1, 2]
}
```

- The `version` field is **required** - all devices must include it
- Current protocol version: **1**

---

## Data Serialization and Chunking

### Problem

The `storage.sync` API has an 8KB per-key size limit. This creates challenges:
1. **Large events**: A single event > 8KB cannot be stored
2. **Large baselines**: A baseline snapshot > 8KB cannot be stored

### Solution: Automatic JSON Serialization with Chunking

The sync engine automatically handles serialization and chunking:

**Automatic Serialization:**
- All data is automatically serialized to JSON strings before storage
- Event handlers receive deserialized data (automatic JSON.parse)
- Baseline handlers receive deserialized state (automatic JSON.parse)
- Users work with native JavaScript objects, serialization is transparent

**Chunking Strategy:**
- If serialized data ≤ 7KB: stored inline (no chunks)
- If serialized data > 7KB: split into chunks and stored in separate keys

### Data Structure

#### Without Chunking (Small Data)

```json
{
  "data": "{\"todos\":{...},\"containers\":{...}}"
}
```

#### With Chunking (Large Data)

**Metadata:**
```json
{
  "chunks": 3
}
```

**Chunk Keys:**
```
key_0: "{\"todos\":{...first 7KB..."
key_1: "...next 7KB..."
key_2: "...remaining data...}}"
```

### Usage Examples

#### Recording Events

```typescript
// User passes native objects
await engine.recordEvent('container:create', {
  name: 'Work',
  color: 'blue',
  icon: 'briefcase'
})

// Engine automatically:
// 1. JSON.stringify() the data
// 2. Chunks if needed (>7KB)
// 3. Stores to storage.sync
```

#### Receiving Events

```typescript
// Handler receives deserialized objects
engine.onApplyEvent((event) => {
  // event.op.data is already a JavaScript object
  console.log(event.op.data.name) // "Work"
  console.log(event.op.data.color) // "blue"
})
```

#### Baselines

```typescript
// Creating baseline - return native object
engine.onCreateBaseline(() => {
  return { todos: {...}, containers: {...} }
})

// Engine automatically:
// 1. JSON.stringify() the state
// 2. Chunks if needed (>7KB)
// 3. Stores to storage.sync

// Loading baseline - receive deserialized object
engine.onApplyBaseline((state) => {
  // state is already a JavaScript object
  appState = state
  renderApp()
})
```

### Chunking Mechanism

**Chunk Naming Pattern:**
```
{base_key_name}_{chunk_index}
```

**Examples:**
- Baseline: `b_uuid`, `b_uuid_0`, `b_uuid_1`, `b_uuid_2`
- Event shard: `e_uuid_5`, `e_uuid_5_0`, `e_uuid_5_1`

**Chunk Indexing:**
- 0-indexed: first chunk is `_0`, second is `_1`, etc.
- Number of chunks stored in metadata

**Reading:**
1. Read metadata from base key
2. If `chunks` field present, read all chunk keys: `{base}_0` to `{base}_{chunks-1}`
3. Concatenate chunks (strings join, ArrayBuffers merge)
4. Return reconstructed data

**Writing:**
1. Check data size
2. If ≤ 7KB: store inline (no chunks)
3. If > 7KB:
   - Split into 7KB chunks
   - Write to separate keys
   - Store metadata with `chunks` count

### Event Sharding with Chunking

**Strategy:**
- Events are stored in arrays per shard: `e_uuid_0`, `e_uuid_1`, etc.
- When adding an event to a shard would exceed 7KB → create new shard
- If a **single event** is > 7KB → the entire shard gets chunked

**Important:** You cannot add events to a shard that is already chunked. A chunked shard is "sealed" - create a new shard for additional events.

**Example Flow:**

1. **Normal case** (small events):
```
e_uuid_0: [event1, event2, event3]  // 3KB total
e_uuid_1: [event4, event5]          // 2KB total
```

2. **Adding event exceeds shard limit** (create new shard):
```
e_uuid_0: [event1, event2, event3]  // 6KB total
// Try to add event4 (2KB) → would exceed 7KB
e_uuid_1: [event4]                  // New shard created
```

3. **Single large event** (shard gets chunked):
```
// event5 is 20KB
e_uuid_2: {
  chunks: 3
}
e_uuid_2_0: "first 7KB of event5..."
e_uuid_2_1: "next 7KB of event5..."
e_uuid_2_2: "remaining data..."
```

### Baseline Chunking

**Strategy:**
- Baseline stored as single key: `b_uuid`
- If baseline > 7KB → automatically chunked

**Example:**

**Small baseline (no chunking):**
```json
b_uuid: {
  includes: { device-A: 100, device-B: 50 },
  state: "{\"todos\":{...},\"containers\":{...}}"
}
```

**Large baseline (with chunking):**
```json
b_uuid: {
  includes: { device-A: 100, device-B: 50 },
  chunks: 4
}
b_uuid_0: "{\"todos\":{...}, \"con..."
b_uuid_1: "tainers\":{...}, \"sit..."
b_uuid_2: "es\":{...}, \"setting..."
b_uuid_3: "s\":{...}}"
```

### Garbage Collection

When removing data with chunks, **all associated chunks must be removed**:

1. Read metadata to check for `chunks` field
2. If chunks present, remove base key + all chunk keys
3. Chunk keys: `{base}_0` to `{base}_{chunks-1}`

**Example:**
```typescript
// Baseline with 3 chunks
const baseline = await storage.get('b_device-A')
if (baseline.chunks) {
  await storage.remove([
    'b_device-A',      // Metadata
    'b_device-A_0',    // Chunk 0
    'b_device-A_1',    // Chunk 1
    'b_device-A_2'     // Chunk 2
  ])
}
```

---

## Hybrid Logical Clock (HLC)

HLC combines physical time with a logical counter to provide total ordering despite clock drift.

When generating an event, if physical time has advanced, use it and reset the counter to 0; otherwise increment the counter. When receiving remote events, adjust local HLC to be higher than any seen timestamp. Events are ordered by: `hlc_time` first, then `hlc_counter`, then `device_id` (extracted from key name) for deterministic tiebreaking.

See the [References](#references) section for detailed information on this algorithm.

---

## Notification Mechanism

`storage.sync` provides an `onChanged` event that fires whenever any key in the sync storage area changes. This includes changes from remote devices after sync propagation (~10 minutes).

When this event is triggered, based on the changed key, we decide the next step:
- `m_*` changes → device generated new events (incremented `last_increment`) → sync to fetch those events
- `e_*` changes → device wrote events → **no sync** (they are updated in sync with `m_*` keys)
- `s_*` changes → device updated vector clock or activity timestamp → **no sync** (avoids loops, doesn't indicate new events)
- `b_*` changes → baseline updated → **no sync** (not necessary, baseline is optimization only)

**Important:** When `m_*` changes, it indicates the remote device has a higher `last_increment`, meaning new events exist. The sync process will compare the new `last_increment` with local `known_increments` to determine which events to fetch.

---

## Operational Flows

### 1. Normal Sync (Happy Path)

**Trigger:** `storage.sync.onChanged` event when any `m_*` key changes.

**Steps:**

1. Fetch all `m_*` keys from storage.sync
2. For each remote device (skip `m_<own_UUID>`):
   - **Validate protocol version:** If device is newly discovered (not in `known_increments`), check `meta.version >= PROTOCOL_VERSION`
   - If `device_id` not in `known_increments`: set `known_increments[device_id] = 0` (new device)
   - Compare `meta.last_increment` with `known_increments[device_id]`
   - If `meta.last_increment > known_increments[device_id]`:
     - Fetch event shards using `meta.shards` array (see [Event Sharding](#5-event-sharding))
     - Filter events where `increment > known_increments[device_id]`
     - Add to processing queue
3. Sort all new events by HLC (time, counter, device_id)
4. Apply events in order
5. Update `known_increments[device_id] = meta.last_increment` for each device
6. Update local HLC from remote events
7. Check if more than 24 hours have passed since last activity update
8. If events were applied OR 24 hours have passed:
   - Update `lastActive` timestamp to current time
   - Persist `s_<own_UUID>` to storage.sync with updated `increments` and `lastActive`

**Note on missing events:** Process only events that are available. If some events are missing (due to GC, corruption, or other reasons), simply skip them. Update `known_increments` based on `meta.last_increment`, not on the highest event actually seen. This ensures continuous progress even when gaps exist.

**Example:**

```
Initial state:
  known_increments = {device_B: 50, device_C: 30}

Sync reads:
  m_B.last_increment = 55
  m_C.last_increment = 35

Actions:
  Fetch events from B where increment > 50 → [51, 52, 55]
  Fetch events from C where increment > 30 → [31, 33, 35]

Sort by HLC:
  [B:51, C:31, B:52, C:33, B:55, C:35]  (example order)

Apply events in order
Update: known_increments = {device_B: 55, device_C: 35}
```

### 2. First Sync / Bootstrap

**When:** New device joining the sync network.

**Steps:**

1. Fetch all `m_*` keys
2. **Validate protocol versions:** For each device's Meta, check `version >= PROTOCOL_VERSION`
3. Look for any device with available baseline (e.g., first device found, device A)
4. **If baseline found:**
   - Fetch `b_A` (baseline snapshot)
   - Apply baseline to local state
   - For each device seen in `m_*`:
     - Fetch event shards using `meta.shards` array (see [Event Sharding](#5-event-sharding))
     - If device is in `b_A.includes`: filter events where `increment > b_A.includes[device_id]`
     - If device NOT in `b_A.includes`: device is newer than baseline, filter events where `increment > 0`
5. **If NO baseline found:**
   - Log warning: no baseline available, will apply all events from scratch
   - For each device seen in `m_*`:
     - Fetch all event shards using `meta.shards` array
     - Collect ALL events where `increment > 0`
6. Sort collected events by HLC
7. Apply events in HLC order
8. Initialize `known_increments` with all devices' `last_increment`
9. Initialize own `last_increment = 0` (next event will be increment 1)
10. Update local HLC: `max(Date.now(), max(all hlc_time from events))`
11. Create initial baseline with current `known_increments` as `includes`
12. Write to storage.sync: meta (with `version = PROTOCOL_VERSION`, `last_increment = 0`, `shards = [0]`), baseline, seen vector `s_<own_UUID>`

**Result:** Device fully synced with fresh baseline, ready to operate.

**Resilience:** If no baseline exists in the network (e.g., all devices lost their baselines), the new device can still join by applying all available events from scratch. This makes the system more resilient to inconsistent states.

### 3. First Device Ever

**When:** User installs extension, no other devices, storage.sync empty.

**Steps:**

1. Detect: no `m_*` keys in storage.sync
2. Generate new UUID
3. Initialize: `last_increment = 0`, `hlc_time = Date.now()`, `hlc_counter = 0`
4. Create initial state locally (e.g., default containers)
5. Write to storage.sync:
   - `m_<UUID>` (with `version = PROTOCOL_VERSION`, `last_increment = 0`, `shards = [0]`)
   - `b_<UUID>` (baseline with initial state, `includes = {}`)
   - **No** `e_<UUID>_0` created yet - events start only when user performs actions
6. Initialize `known_increments = {}`

**Result:** First device initialized with baseline containing initial state. No events yet - history starts when user takes first action.

**Rationale:** The first device doesn't need events because there's no prior state to track. The baseline represents the "starting point" and events will be created only when actual changes occur.

### 4. Local Event Write

**Trigger:** User action (create/modify/delete container, site assignment).

**Flow:**

**Application side (BEFORE calling recordEvent):**
1. Apply operation to application state (e.g., add container to local state)
2. Call `syncEngine.recordEvent(type, data)`

**SyncEngine.recordEvent() (records the event for sync):**
1. Calculate new increment:
   ```
   new_increment = device_state.last_increment + 1
   ```
2. Advance HLC: if `Date.now() > hlc_time`, set `hlc_time = Date.now()` and `hlc_counter = 0`; otherwise increment `hlc_counter`
3. Create event:
   ```json
   {
     "increment": new_increment,
     "hlc_time": hlc_time,
     "hlc_counter": hlc_counter,
     "op": {type, data}
   }
   ```
4. Append event to appropriate shard `e_<UUID>_<X>` (see [Event Sharding](#5-event-sharding))
5. Increment `events_since_baseline_update`
6. If `events_since_baseline_update >= 15`:
   - Serialize current state to `b_<UUID>` with updated `includes`:
     - `includes[own_device_id] = new_increment`
     - `includes[other_devices]` = current values from `known_increments`
   - Reset `events_since_baseline_update = 0`
7. Write to storage.sync (atomic):
   - Update `e_<UUID>_<X>` (append event)
   - Update `m_<UUID>` (with `version = PROTOCOL_VERSION`, new last_increment, update shards array if new shard created)
   - Update `b_<UUID>` (if threshold reached)

**Important:** `recordEvent()` does NOT apply the event to application state. The application must apply the operation BEFORE calling `recordEvent()`. The SyncEngine only records the event for synchronization with other devices.

**Note:** The `onChanged` event will fire on all other devices after sync propagation.

### 5. Event Sharding

**Problem:** storage.sync has 8KB per-key limit.

**Strategy:**

**Read:**
- Fetch `m_<UUID>` to get `shards` array
- For each X in `shards`: fetch `e_<UUID>_X`
- Concatenate all event arrays
- Filter by increment if needed

**Write:**
- Calculate serialized event size: `JSON.stringify(event).length`
- Fetch current shard `e_<UUID>_<current_shard>`
- Calculate current shard size
- If `current_size + event_size > 7000` bytes (buffer for safety):
  - Increment: `current_shard++`
  - Add to meta: `meta.shards.push(current_shard)`
  - Write event to `e_<UUID>_<current_shard>`
- Else append to current shard

**Tracking:** Store `current_shard` in `device_state`.

### 6. Garbage Collection

**Goal:** Delete old events seen by all devices to prevent storage.sync quota exhaustion.

**Important:** Each device is responsible for garbage collecting **only its own events** (`e_<own_UUID>_*`). No device deletes events created by other devices. This follows the principle of no shared writable keys.

**Strategy: Distributed Watermark**

**Update seen vector (during sync):**

After processing events from device X, update:
```
s_<own_UUID>.increments[device_X] = m_X.last_increment
```
Write updated `s_<own_UUID>` to storage.sync (includes both `increments` map and `lastActive` timestamp).

**Garbage Collection (periodic, e.g., every 10 syncs):**

Each device performs GC to clean up its own events:

1. Fetch all `b_*` baseline keys from storage
2. For each baseline, collect `baseline.includes[own_device_id]`
3. Calculate safe removal threshold:
   - If no baselines exist: `safeToRemove = last_increment` (remove everything)
   - Otherwise: `safeToRemove = min(all baseline.includes[own_device_id])`
4. This ensures we only remove events that ALL devices have already included in their baseline
   - If baseline B only includes up to event 60 from device A, device A cannot safely remove events > 60
5. Collect all own events from shards `e_<own_UUID>_*`
6. Keep only events where `increment > safeToRemove`
7. Redistribute kept events into new shards (max ~7KB per shard, staying under the 8KB storage.sync limit)
8. Delete empty shards from storage using `storage.remove(key)`
9. Update `m_<own_UUID>` (with `version = PROTOCOL_VERSION`) with new shard list

**Why baselines determine GC:**
- New devices bootstrap from any available baseline
- If baseline `b_B` has `includes[device_A] = 60`, new devices need events > 60 from device A
- Device A must keep events > 60 even if device A itself has moved ahead
- Only when ALL baselines include an event can it be safely removed
- Offline devices are automatically protected: their old baseline blocks GC of events they still need

### 7. Inactive Device Removal

**Goal:** Remove devices that have been inactive for an extended period to free storage quota and prevent accumulation of abandoned device data.

**Configuration:**
- `removeInactiveDevices`: Boolean flag to enable/disable the feature (default: false, opt-in)
- `inactiveDeviceTimeout`: Timeout in milliseconds after which a device is considered inactive (default: 60 days)

**Activity Tracking:**

Each device updates its `s_<UUID>.lastActive` timestamp once per day during sync:
1. Check if more than 24 hours have passed since last activity update
2. If yes, update `lastActive` to current timestamp
3. Write updated `s_<UUID>` to storage.sync

This daily update serves as a heartbeat signal, allowing other devices to detect when a device has stopped syncing.

**Removal Process (during GC, if enabled):**

1. Fetch all `m_*` and `s_*` keys from storage
2. For each remote device:
   - Read its `s_<device_id>.lastActive` timestamp
   - If `(now - lastActive) > inactiveDeviceTimeout`:
     - Mark device as inactive for removal
3. For each inactive device, remove all its keys:
   - `m_<device_id>` (meta)
   - `b_<device_id>` (baseline)
   - `s_<device_id>` (seen vector)
   - All `e_<device_id>_*` (event shards, using `meta.shards` to identify them)
4. Remove inactive device IDs from own `known_increments`
5. Update own `s_<own_UUID>` with cleaned `increments` map

**Device Rejoining:**

If a device returns after being removed:
1. Device finds no `m_<own_UUID>` during initialization
2. Device performs bootstrap as a new device (same flow as [First Sync / Bootstrap](#2-first-sync--bootstrap))
3. Device fetches baseline and events from remaining devices
4. Device creates new `m_*`, `b_*`, `s_*` keys with initial state

**Safety:**
- Opt-in by default to prevent accidental data loss
- Long timeout (60 days default) to avoid removing temporarily offline devices
- Device can rejoin seamlessly by bootstrapping from remaining devices
- No data loss for other devices - only the inactive device's own sync metadata is removed

---

## Conflict Resolution

### Principles

1. **Delete wins:** If container is deleted, subsequent modifications are ignored
2. **HLC ordering:** Events applied in strict HLC order
3. **Replace semantics:** Modify operations replace entire object (not field-level merge)

### Scenarios

#### Delete vs Modify

```
Events sorted by HLC:
1. Device A: modify container X {color: "blue"} (HLC 100)
2. Device B: delete container X (HLC 105)
3. Device C: modify container X {name: "Work"} (HLC 110)

Application:
- Apply modify (100): X exists, color = blue
- Apply delete (105): X removed
- Apply modify (110): X doesn't exist → IGNORED

Result: X does not exist
```

#### Concurrent Modify

```
Events:
- Device A: modify X {color: "blue"} (HLC 100)
- Device B: modify X {name: "Work"} (HLC 101)

Application (replace semantics):
- Apply A (100): X = {color: "blue"}
- Apply B (101): X = {name: "Work"}  (color lost!)

Result: X = {name: "Work"}
```

---

## References

- Hybrid Logical Clocks (paper): https://www.semanticscholar.org/paper/Logical-Physical-Clocks-and-Consistent-Snapshots-in-Demirbas-Leone/3179127ec316644bfc4643d0263c306f890c1d36
- Hybrid Clock (Martin Fowler): https://martinfowler.com/articles/patterns-of-distributed-systems/hybrid-clock.html
- Logical clock (Wikipedia): https://en.wikipedia.org/wiki/Logical_clock
- Vector Clocks (Wikipedia): https://en.wikipedia.org/wiki/Vector_clock
- Event Sourcing: https://martinfowler.com/eaaDev/EventSourcing.html
